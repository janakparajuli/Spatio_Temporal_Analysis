{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f18ec02-1331-4570-ab5e-d7d49506ff11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "250b354b-ef6d-4a65-b9d9-48f8b6966af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, Dense, BatchNormalization, MaxPooling3D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f830b785-0a56-4646-9b69-5c33dac6d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from CSV file\n",
    "data = pd.read_csv(r\"./Daily_HAI_Mean.csv\")\n",
    "\n",
    "# Preprocess the data\n",
    "data['date'] = pd.to_datetime(data['ds'])\n",
    "data['latitude'] = (data['Lat'] - data['Lat'].min()) / (data['Lat'].max() - data['Lat'].min())\n",
    "data['longitude'] = (data['Lon'] - data['Lon'].min()) / (data['Lon'].max() - data['Lon'].min())\n",
    "data['activity_index'] = (data['y'] - data['y'].min()) / (data['y'].max() - data['y'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db78f33d-7a91-4ec3-ab54-35076f42716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad21e737-4b67-418e-9f1d-69c3277f09c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets (considering dates)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define parameters\n",
    "seq_length = 7  # Length of sequence for each sample\n",
    "n_features = 2  # Number of features (latitude, longitude, activity_index)\n",
    "n_filters = 4  # Number of filters in ConvLSTM layers\n",
    "kernel_size = (1, 1)  # Kernel size for ConvLSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b61c36a-21e4-4cce-8100-a9f775527a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61f971ad-1b45-4229-83d0-482d813e9d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for ConvLSTM\n",
    "def prepare_data(data, include_target=True):\n",
    "    X = []\n",
    "    y = []\n",
    "    data = data.sort_values(by='date')  # Sort data by date\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[['latitude', 'longitude']].iloc[i:i+seq_length].values)\n",
    "        if include_target:\n",
    "            y.append(data['activity_index'].iloc[i+seq_length])\n",
    "    X = np.array(X)\n",
    "    #print(X)\n",
    "    if include_target:\n",
    "        y = np.array(y)\n",
    "        return X, y\n",
    "    return X, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05d850f-02fa-4c34-bbc8-2d323f401299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21e50ace-63bc-4b82-b90f-417fa493b33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before reshaping:\n",
      "X_train shape: (285, 7, 2)\n",
      "X_test shape: (67, 7, 2)\n",
      "After reshaping:\n",
      "X_train shape: (285, 7, 1, 2, 1)\n",
      "X_test shape: (67, 7, 1, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = prepare_data(train_data)\n",
    "X_test, y_test = prepare_data(test_data)\n",
    "\n",
    "# Reshape the data for ConvLSTM\n",
    "print(\"Before reshaping:\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "#print(X_train)\n",
    "X_train = X_train.reshape(-1, seq_length, 1, n_features, 1)\n",
    "X_test = X_test.reshape(-1, seq_length, 1, n_features, 1)\n",
    "\n",
    "print(\"After reshaping:\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35dd8263-ab36-4850-b18f-a8033f613fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "507466c0-bdb3-44ef-b482-52965d3d6452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the ConvLSTM model\n",
    "model = Sequential([\n",
    "    ConvLSTM2D(filters=n_filters, kernel_size=kernel_size, activation='relu', input_shape=(seq_length, 1, n_features, 1), return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    #MaxPooling3D(pool_size=(1, 1, 1)),\n",
    "    \n",
    "    ConvLSTM2D(filters=2*n_filters, kernel_size=kernel_size, activation='relu', return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    #MaxPooling3D(pool_size=(1, 1, 1)),\n",
    "    \n",
    "    ConvLSTM2D(filters=4*n_filters, kernel_size=kernel_size, activation='relu', return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    #MaxPooling3D(pool_size=(1, 1, 1)),\n",
    "    \n",
    "    ConvLSTM2D(filters=8*n_filters, kernel_size=kernel_size, activation='relu', return_sequences=False),\n",
    "    BatchNormalization(),\n",
    "    #MaxPooling3D(pool_size=(1, 1, 1)),\n",
    "    \n",
    "    #ConvLSTM2D(filters=16*n_filters, kernel_size=kernel_size, activation='relu'),\n",
    "    #BatchNormalization(),\n",
    "    #MaxPooling3D(pool_size=(1, 1, 1)),\n",
    "\n",
    "    Flatten(),  # Flatten the output before the Dense layer\n",
    "    \n",
    "    Dense(1)  # Output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1494c5f6-599c-4176-ab07-874696f2faf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_lstm2d_4 (ConvLSTM2D)  (None, 7, 1, 2, 4)        96        \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 7, 1, 2, 4)        16        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv_lstm2d_5 (ConvLSTM2D)  (None, 7, 1, 2, 8)        416       \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 7, 1, 2, 8)        32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv_lstm2d_6 (ConvLSTM2D)  (None, 7, 1, 2, 16)       1600      \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 7, 1, 2, 16)       64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv_lstm2d_7 (ConvLSTM2D)  (None, 1, 2, 32)          6272      \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 1, 2, 32)          128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8689 (33.94 KB)\n",
      "Trainable params: 8569 (33.47 KB)\n",
      "Non-trainable params: 120 (480.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65b92a84-1b5a-473d-9472-49e7a1fd849b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "9/9 [==============================] - 9s 108ms/step - loss: 0.4732 - val_loss: 0.1565\n",
      "Epoch 2/25\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.2031 - val_loss: 0.1039\n",
      "Epoch 3/25\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.1106 - val_loss: 0.0652\n",
      "Epoch 4/25\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0840 - val_loss: 0.0421\n",
      "Epoch 5/25\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0594 - val_loss: 0.0318\n",
      "Epoch 6/25\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0514 - val_loss: 0.0299\n",
      "Epoch 7/25\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0383 - val_loss: 0.0344\n",
      "Epoch 8/25\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0337 - val_loss: 0.0428\n",
      "Epoch 9/25\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0315 - val_loss: 0.0546\n",
      "Epoch 10/25\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.0276 - val_loss: 0.0673\n",
      "Epoch 11/25\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0247 - val_loss: 0.0792\n",
      "Epoch 12/25\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.0259 - val_loss: 0.0936\n",
      "Epoch 13/25\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0297 - val_loss: 0.1060\n",
      "Epoch 14/25\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0244 - val_loss: 0.1272\n",
      "Epoch 15/25\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0236 - val_loss: 0.1400\n",
      "Epoch 16/25\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0221 - val_loss: 0.1593\n",
      "Epoch 17/25\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.0226 - val_loss: 0.1770\n",
      "Epoch 18/25\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.0229 - val_loss: 0.2055\n",
      "Epoch 19/25\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.0183 - val_loss: 0.2208\n",
      "Epoch 20/25\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0195 - val_loss: 0.2382\n",
      "Epoch 21/25\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0181 - val_loss: 0.2660\n",
      "Epoch 22/25\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0178 - val_loss: 0.2902\n",
      "Epoch 23/25\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0191 - val_loss: 0.3065\n",
      "Epoch 24/25\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.0185 - val_loss: 0.3402\n",
      "Epoch 25/25\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.0168 - val_loss: 0.3641\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "batch_size = 32\n",
    "model.compile(optimizer=Adam(), loss='mse')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=25, batch_size=batch_size, validation_data=(X_test, y_test), verbose = 1)\n",
    "# Define the filename with hyperparameters\n",
    "filename = f\"ConvLSTM_model_filters_{n_filters}_kernel_{kernel_size}_seq_{seq_length}_batch_size_{batch_size}.h5\"\n",
    "\n",
    "# Save the model\n",
    "model.save(r\"./outputs/models/\"+filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddc784b-7cf5-4fcb-a9d5-89e02a285c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss vs validation loss\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043eea63-b308-4816-8cf1-11191daafb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)#.reshape(-1)\n",
    "print(y_pred.shape)\n",
    "#y_pred = y_pred[:, 0, 0]\n",
    "#print(y_pred.shape)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(mse, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1c94bd-1eaa-4119-8cb4-acc48a737c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4051c0-9476-47d7-86cf-e886dc462381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = y_pred[:, 0]\n",
    "y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31359df8-8f97-49c9-bee8-fcc50f659c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred[:,0], color='blue')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals\n",
    "residuals = y_test - y_pred[:,0]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, residuals, color='blue')\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals Plot')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214727c1-06c7-4320-8a83-b76af71588bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test data and predicted values to CSV\n",
    "test_df = pd.DataFrame({'Actual_Index': y_test, 'Pred_Index': y_pred[:,0],\n",
    "                        'Latitude': test_data['Lat'][-len(y_test):].values,\n",
    "                        'Longitude': test_data['Lon'][-len(y_test):].values,\n",
    "                        'Date': test_data['date'][-len(y_test):].dt.strftime('%Y-%m-%d')})  # Convert datetime to string\n",
    "test_df.to_csv('./outputs/csv/test_pred_values.csv', index=False)\n",
    "\n",
    "# Convert CSV to shapefile\n",
    "geometry = [Point(xy) for xy in zip(test_df['Longitude'], test_df['Latitude'])]\n",
    "gdf = gpd.GeoDataFrame(test_df, geometry=geometry)\n",
    "gdf.crs = {'init': 'epsg:4326'}  # Assuming WGS84 coordinate reference system\n",
    "gdf.to_file('./outputs/shp/test_predicted_values.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feaf786-bd3f-4405-ba0b-8213eeb77c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random data for prediction\n",
    "random_data = pd.DataFrame({\n",
    "    'date': pd.date_range(start='2020-11-01', end='2020-11-25'),\n",
    "#    'latitude': np.random.uniform(low=data['Lat'].min(), high=(data['Lat'].max()), size=(61,)),\n",
    "#    'longitude': np.random.uniform(low=(data['Lon'].min()), high=(data['Lon'].max()), size=(61,))\n",
    "    'latitude': np.random.uniform(low=50, high=52, size=(25,)),\n",
    "    'longitude': np.random.uniform(low=7, high=8, size=(25,))\n",
    "})\n",
    "random_data.to_csv(r'./outputs/csv/random_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b7e60-d466-4c27-8545-225515f09062",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 2\n",
    "random_data = pd.read_csv(r'./outputs/csv/random_data.csv')\n",
    "# Normalize the random data\n",
    "random_data['latitude'] = (random_data['latitude'] - random_data['latitude'].min()) / (random_data['latitude'].max() - random_data['latitude'].min())\n",
    "random_data['longitude'] = (random_data['longitude'] - random_data['longitude'].min()) / (random_data['longitude'].max() - random_data['longitude'].min())\n",
    "#random_data['activity_index'] = np.nan  # Placeholder for activity index\n",
    "random_data.to_csv(r'./outputs/csv/random_data_normalized.csv', index=False)\n",
    "\n",
    "# Prepare random data for prediction\n",
    "X_random, _ = prepare_data(random_data, include_target=False)\n",
    "\n",
    "#print(type(X_random))\n",
    "#print(X_random)\n",
    "X_random = X_random.reshape(-1, seq_length, 1, n_features, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ff1a9-6cf7-4bf4-bf38-f6b4ffcdc332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_random\n",
    "#X_df = pd.DataFrame(X_random.reshape(25,1))\n",
    "#X_df.to_csv(r'./outputs/X_random.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa2f596-5540-4dc7-97fd-eb6346866bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on random data\n",
    "y_random_pred = model.predict(X_random)#.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4de420-e793-4cc2-af64-3d9d44622021",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717e9f9d-4f50-4864-a2eb-9e247257d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_random_pred = y_random_pred#[:,0]\n",
    "(X_random.shape), (y_random_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d33aa5-6eda-4cf3-8e87-6b83ebced93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_data['date'] = pd.to_datetime(random_data['date'])\n",
    "\n",
    "# Combine random data with predicted values\n",
    "random_pred_df = pd.DataFrame({\n",
    "    'Pred_Index': y_random_pred,\n",
    "    'Latitude': random_data['latitude'].values[-len(y_random_pred):],\n",
    "    'Longitude': random_data['longitude'].values[-len(y_random_pred):],\n",
    "    'Date': random_data['date'].iloc[-len(y_random_pred):].dt.strftime('%Y-%m-%d')\n",
    "})\n",
    "\n",
    "# Save random data and predicted values to CSV\n",
    "random_pred_df.to_csv('./outputs/csv/random_pred_values.csv', index=False)\n",
    "\n",
    "# Convert random data CSV to shapefile\n",
    "geometry_random = [Point(xy) for xy in zip(random_pred_df['Longitude'], random_pred_df['Latitude'])]\n",
    "gdf_random = gpd.GeoDataFrame(random_pred_df, geometry=geometry_random)\n",
    "gdf_random.crs = {'init': 'epsg:4326'}  # Assuming WGS84 coordinate reference system\n",
    "gdf_random.to_file('./outputs/shp/random_predicted_values.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10478570-3ecd-4f5d-b607-d3ae14a5e6dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cc8425-e4ff-4469-8e4d-5ba92372ef49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee7629d-c5bb-4ad7-90bf-688eae532bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39979d5-b2df-4d70-a17d-d59ddcdfdcf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109ff45b-38b0-41e7-b558-c80f03832b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization, MaxPooling2D\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Load the data from CSV file\n",
    "data = pd.read_csv(r\"./Daily_HAI_Mean.csv\")\n",
    "\n",
    "# Preprocess the data\n",
    "data['date'] = pd.to_datetime(data['ds'])\n",
    "data['latitude'] = (data['Lat'] - data['Lat'].min()) / (data['Lat'].max() - data['Lat'].min())\n",
    "data['longitude'] = (data['Lon'] - data['Lon'].min()) / (data['Lon'].max() - data['Lon'].min())\n",
    "data['activity_index'] = (data['y'] - data['y'].min()) / (data['y'].max() - data['y'].min())\n",
    "\n",
    "# Split data into train and test sets (considering dates)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define parameters\n",
    "seq_length = 7  # Length of sequence for each sample\n",
    "n_features = 3  # Number of features (latitude, longitude, activity_index)\n",
    "n_filters = 32  # Number of filters in ConvLSTM layers\n",
    "kernel_size = (1, 1)  # Kernel size for ConvLSTM layers\n",
    "\n",
    "# Prepare the data for ConvLSTM\n",
    "def prepare_data(data):\n",
    "    X = []\n",
    "    y = []\n",
    "    data = data.sort_values(by='date')  # Sort data by date\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[['latitude', 'longitude', 'activity_index']].iloc[i:i+seq_length].values)\n",
    "        y.append(data['activity_index'].iloc[i+seq_length])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = prepare_data(train_data)\n",
    "X_test, y_test = prepare_data(test_data)\n",
    "\n",
    "X_train, y_train = prepare_data(train_data)\n",
    "X_test, y_test = prepare_data(test_data)\n",
    "\n",
    "# Reshape the data for ConvLSTM\n",
    "print(\"Before reshaping:\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "X_train = X_train.reshape(-1, seq_length, 1, n_features, 1)\n",
    "X_test = X_test.reshape(-1, seq_length, 1,  n_features, 1)\n",
    "\n",
    "print(\"After reshaping:\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "# Build the ConvLSTM model\n",
    "model = Sequential([\n",
    "    ConvLSTM2D(filters=n_filters, kernel_size=kernel_size, activation='relu', input_shape=(seq_length, 1, n_features, 1), return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    #MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    ConvLSTM2D(filters=64, kernel_size=kernel_size, activation='relu', return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    #MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    ConvLSTM2D(filters=128, kernel_size=kernel_size, activation='relu', return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    #MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    ConvLSTM2D(filters=256, kernel_size=kernel_size, activation='relu', return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    #MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    ConvLSTM2D(filters=512, kernel_size=kernel_size, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    #MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Dense(1)  # Output layer\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='mse')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test).reshape(-1)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Plot loss vs validation loss\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot actual vs predicted\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, color='blue')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, residuals, color='blue')\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals Plot')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Save test data and predicted values to CSV\n",
    "test_df = pd.DataFrame({'Actual_Activity_Index': y_test, 'Predicted_Activity_Index': y_pred,\n",
    "                        'Latitude': test_data['Lat'][-len(y_test):].values,\n",
    "                        'Longitude': test_data['Lon'][-len(y_test):].values,\n",
    "                        'Date': test_data['date'][-len(y_test):].values})\n",
    "test_df.to_csv('test_predicted_values.csv', index=False)\n",
    "\n",
    "# Convert CSV to shapefile\n",
    "geometry = [Point(xy) for xy in zip(test_df['Longitude'], test_df['Latitude'])]\n",
    "gdf = gpd.GeoDataFrame(test_df, geometry=geometry)\n",
    "gdf.crs = {'init': 'epsg:4326'}  # Assuming WGS84 coordinate reference system\n",
    "gdf.to_file('test_predicted_values.shp')\n",
    "\n",
    "# Generate random data for prediction\n",
    "random_data = pd.DataFrame({\n",
    "    'Date': pd.date_range(start='2023-01-01', end='2023-01-10'),\n",
    "    'Lat': np.random.uniform(low=data['Lat'].min(), high=data['Lat'].max(), size=(10,)),\n",
    "    'Lon': np.random.uniform(low=data['Lon'].min(), high=data['Lon'].max(), size=(10,))\n",
    "})\n",
    "\n",
    "# Prepare random data for prediction\n",
    "X_random, _ = prepare_data(random_data)\n",
    "X_random = X_random.reshape(-1, seq_length, 1, n_features, 1)\n",
    "\n",
    "# Make predictions on random data\n",
    "y_random_pred = model.predict(X_random).reshape(-1)\n",
    "\n",
    "# Combine random data with predicted values\n",
    "random_pred_df = pd.DataFrame({'Predicted_Activity_Index': y_random_pred,\n",
    "                               'Latitude': random_data['Lat'].values,\n",
    "                               'Longitude': random_data['Lon'].values,\n",
    "                               'Date': random_data['Date'].values})\n",
    "\n",
    "# Save random data and predicted values to CSV\n",
    "random_pred_df.to_csv('random_predicted_values.csv', index=False)\n",
    "\n",
    "# Convert random data CSV to shapefile\n",
    "geometry_random = [Point(xy) for xy in zip(random_pred_df['Longitude'], random_pred_df['Latitude'])]\n",
    "gdf_random = gpd.GeoDataFrame(random_pred_df, geometry=geometry_random)\n",
    "gdf_random.crs = {'init': 'epsg:4326'}  # Assuming WGS84 coordinate reference system\n",
    "gdf_random.to_file('random_predicted_values.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5195c31b-9a5d-4f08-aeb4-086e564e402b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4860676d-6147-4b6f-9c5e-32e5da8cdbc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ff53d2-51b5-4680-b7c9-d483f76d6fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602080ed-bcf5-45ae-96e9-6cb5a8939d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5897bd-ce8d-4668-99d1-76de94f3f1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7116f1a-0231-480b-9bd2-402bd79d5413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bd64cf-099b-4aa2-b696-f3de48b7d81a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47486516-5e84-4a0b-95a8-683945fbed4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3577ce-94a4-4ffe-98fa-d40e805f5a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287fda99-a023-4a93-b523-504fe31ad3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05c6ca6-b814-48dc-a42e-a2de8a2355cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
